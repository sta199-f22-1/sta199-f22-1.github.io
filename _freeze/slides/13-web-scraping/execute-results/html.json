{
  "hash": "d1d35d7cb91d4e05a80eb3012f4494a4",
  "result": {
    "markdown": "---\ntitle: \"Web scraping\"\nsubtitle: \"Lecture 13\"\ndate: \"October 13, 2022\"\nformat: revealjs\n---\n\n\n# Warm up\n\n\n::: {.cell}\n\n:::\n\n\n## While you wait for class to begin...\n\n::: nonincremental\n-   If you haven't yet done so: Install a Chrome browser and the SelectorGadget extension:\n    -   [Chrome](https://www.google.com/chrome/)\n    -   [SelectorGadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en)\n-   Clone your `ae-10` project from GitHub, render your document, update your name, and commit and push.\n:::\n\n## Announcements\n\n-   Watch out for announcement on team assignments before Monday's lab\n-   Start thinking about project ideas to bring to your team\n-   HW 3 is due at 11:59 on Thursday\n\n## Midterm feedback I {.smaller}\n\n::: incremental\n-   Thank you to the 49 of you who filled it out!\n-   **How would you change the class:** Overwhelming response: More live coding\n-   **Pace**:\n    -   78% About right\n    -   6% Too fast\n    -   4% Too slow\n    -   12% No response\n-   **Number of hours spent outside of class:**\n    -   Avg 5.5 hours (Expected \\~6.25 hours)\n    -   If you're consistently spending less and not doing well, you should be putting more time in\n    -   If you're consistently spending more and feel like it's not working, reach out for help\n:::\n\n## Midterm feedback II {.smaller}\n\n::: incremental\n-   **How are you learning best:**\n    -   AEs during class\n    -   Assignments\n    -   Videos\n-   **TAs:**\n    -   Overwhelmingly positive!\n    -   If you're not making use of the TA resources, your peers think you should!\n:::\n\n## Midterm feedback III {.smaller}\n\n**General feedback:**\n\n::: incremental\n-   **Positive:** Generally, people like the interactive nature of the class, the live coding aspect, and the topics\n\n-   **Constructive:**\n\n    -   Lessen overlap between prep materials -- Can't lessen since people learn differently, but I can annotate the overlap. You should also feel free to go through pieces at a faster pace if they feel redundant.\n    -   Can be difficult to catch up on AEs if bits are missed -- Ask on Slack or come by office hours, no need to wait for answer keys to be posted.\n    -   Sometimes grading feels harsh -- Ask for clarification on points taken if not provided and don't hesitate to ask for a regrade if errors are made in grading.\n    -   We haven't done any statistics -- All of this is (modern) statistics!\n:::\n\n# Web scraping\n\n## Scraping the web: what? why? {.smaller}\n\n-   Increasing amount of data is available on the web\n\n-   These data are provided in an unstructured format: you can always copy&paste, but it's time-consuming and prone to errors\n\n-   Web scraping is the process of extracting this information automatically and transform it into a structured dataset\n\n-   Two different scenarios:\n\n    -   Screen scraping: extract data from source code of website, with html parser (easy) or regular expression matching (less easy).\n\n    -   Web APIs (application programming interface): website offers a set of structured http requests that return JSON or XML files.\n\n## Hypertext Markup Language\n\n-   Most of the data on the web is still largely available as HTML\n-   It is structured (hierarchical / tree based), but it's often not available in a form useful for analysis (flat / tidy).\n\n``` html\n<html>\n  <head>\n    <title>This is a title</title>\n  </head>\n  <body>\n    <p align=\"center\">Hello world!</p>\n  </body>\n</html>\n```\n\n## rvest\n\n::: columns\n::: {.column width=\"50%\"}\n-   The **rvest** package makes basic processing and manipulation of HTML data straight forward\n-   It's designed to work with pipelines built with `|>`\n-   [rvest.tidyverse.org](https://rvest.tidyverse.org)\n:::\n\n::: {.column width=\"50%\"}\n[![](images/13/rvest.png){fig-alt=\"rvest hex logo\" fig-align=\"right\" width=\"400\"}](https://rvest.tidyverse.org/)\n:::\n:::\n\n## Core rvest functions {.smaller}\n\n-   `read_html()` - Read HTML data from a url or character string (actually from the **xml2** package, but most often used along with other rvest functions)\n-   `html_element()` / `html_elements()` - Select a specified element(s) from HTML document\n-   `html_table()` - Parse an HTML table into a data frame\n-   `html_text()` - Extract text from an element\n-   `html_text2()` - Extract text from an element and lightly format it to match how text looks in the browser\n-   `html_name()` - Extract elements' names\n-   `html_attr()` / `html_attrs()` - Extract a single attribute or all attributes\n\n# Application exercise\n\n## Opinion articles in The Chronicle\n\n-   Go to <https://www.dukechronicle.com/section/opinion>\n-   Scroll to the bottom and choose page 1\n\n::: question\n::: nonincremental\n-   How many articles are on the page?\n-   Take a look at the URL. How can you change the number of articles displayed by modifying the URL? Try displaying 100 articles.\n:::\n:::\n\n## Goal\n\n::: columns\n::: {.column width=\"50%\"}\n-   Scrape data and organize it in a tidy format in R\n-   Perform light text parsing to clean data\n-   Summarize and visualze the data\n:::\n\n::: {.column width=\"50%\"}\n![](images/13/chronicle-data.png){fig-align=\"center\"}\n:::\n:::\n\n## `ae-10`\n\n::: appex\n::: nonincremental\n-   Go to the course [GitHub org](https://github.com/sta199-f22-1) and find your `ae-10` (repo name will be suffixed with your GitHub name).\n-   Clone the repo in your container, open the Quarto document in the repo, and follow along and complete the exercises.\n-   Render, commit, and push your edits by the AE deadline -- 3 days from today.\n:::\n:::\n\n## Recap\n\n-   Use the SelectorGadget identify tags for elements you want to grab\n-   Use rvest to first read the whole page (into R) and then parse the object you've read in to the elements you're interested in\n-   Put the components together in a data frame (a tibble) and analyze it like you analyze any other data\n\n## A new R workflow {.smaller}\n\n-   When working in a Quarto document, your analysis is re-run each time you knit\n\n-   If web scraping in a Quarto document, you'd be re-scraping the data each time you knit, which is undesirable (and not *nice*)!\n\n-   An alternative workflow:\n\n    -   Use an R script to save your code\n    -   Saving interim data scraped using the code in the script as CSV or RDS files\n    -   Use the saved data in your analysis in your Quarto document\n\n# Web scraping considerations\n\n## Ethics: \"Can you?\" vs \"Should you?\"\n\n![](images/13/ok-cupid-1.png){fig-align=\"center\" width=\"800\"}\n\n::: aside\nSource: Brian Resnick, [Researchers just released profile data on 70,000 OkCupid users without permission](https://www.vox.com/2016/5/12/11666116/70000-okcupid-users-data-release), Vox.\n:::\n\n## \"Can you?\" vs \"Should you?\"\n\n![](images/13/ok-cupid-2.png){fig-align=\"center\" width=\"800\"}\n\n## Challenges: Unreliable formatting\n\n![](images/13/unreliable-formatting.png){fig-align=\"center\" width=\"699\"}\n\n::: aside\n[alumni.duke.edu/news/notable-alumni](https://alumni.duke.edu/news/notable-alumni)\n:::\n\n## Challenges: Data broken into many pages\n\n![](images/13/many-pages.png){fig-align=\"center\"}\n\n## Workflow: Screen scraping vs. APIs\n\nTwo different scenarios for web scraping:\n\n-   Screen scraping: extract data from source code of website, with html parser (easy) or regular expression matching (less easy)\n\n-   Web APIs (application programming interface): website offers a set of structured http requests that return JSON or XML files\n",
    "supporting": [
      "13-web-scraping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}